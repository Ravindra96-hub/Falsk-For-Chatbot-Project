{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\navya\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "# word stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import json\n",
    "training_data = []\n",
    "with open(r\"intents.json\", encoding=\"utf8\") as f:\n",
    "    reader = json.load(f)\n",
    "read = [{}]\n",
    "dicti = {}\n",
    "j=0\n",
    "for i in reader['intents']:\n",
    "    for k in i['patterns']:\n",
    "        dicti = {\"class\":i[\"tag\"],\"sentence\":k}\n",
    "        training_data.append(dicti)\n",
    "#print(training_data)\n",
    "\n",
    "#print(len(training_data))\n",
    "\n",
    "# capture unique stemmed words in the training corpus\n",
    "corpus_words = {}\n",
    "class_words = {}\n",
    "# turn a list into a set (of unique items) and then a list again (this removes duplicates)\n",
    "classes = set([a['class'] for a in training_data])\n",
    "\n",
    "for c in classes:\n",
    "    # prepare a list of words within each class\n",
    "    class_words[c] = []\n",
    "\n",
    "# loop through each sentence in our training data\n",
    "for data in training_data:\n",
    "    # tokenize each sentence into words\n",
    "    for word in nltk.word_tokenize(data['sentence']):\n",
    "        # ignore a some things\n",
    "        if word not in [\"?\", \"'s\"]:\n",
    "            # stem and lowercase each word\n",
    "            stemmed_word = stemmer.stem(word.lower())\n",
    "            # have we not seen this word already?\n",
    "            if stemmed_word not in corpus_words:\n",
    "                corpus_words[stemmed_word] = 1\n",
    "            else:\n",
    "                corpus_words[stemmed_word] += 1\n",
    "            # add the word to our words in class list\n",
    "            class_words[data['class']].extend([stemmed_word])\n",
    "\n",
    "# we now have each stemmed word and the number of occurances of the word in our training corpus (the word's commonality)\n",
    "#print (\"Corpus words and counts: %s \\n\" % corpus_words)\n",
    "# also we have all words in each class\n",
    "#print (\"Class words: %s\" % class_words)\n",
    "\n",
    "#calculate a score for a given class taking into account word commonality\n",
    "def calculate_class_score(sentence, class_name, show_details=True):\n",
    "    score = 0\n",
    "    # tokenize each word in our new sentence\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        # check to see if the stem of the word is in any of our classes\n",
    "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
    "            # treat each word with relative weight\n",
    "            score += (1 / corpus_words[stemmer.stem(word.lower())])\n",
    "            if show_details:\n",
    "                print (\"   match: %s (%s)\" % (stemmer.stem(word.lower()), 1 / corpus_words[stemmer.stem(word.lower())]))\n",
    "    return score\n",
    "\n",
    "def calculate_class_score_commonality(sentence, class_name, show_details=True):\n",
    "    score = 0\n",
    "    # tokenize each word in our new sentence\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        # check to see if the stem of the word is in any of our classes\n",
    "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
    "            # treat each word with relative weight\n",
    "            score += (1 / corpus_words[stemmer.stem(word.lower())])\n",
    "            if show_details:\n",
    "                print (\"match: %s (%s)\" % (stemmer.stem(word.lower()), 1 / corpus_words[stemmer.stem(word.lower())]))\n",
    "    return score\n",
    "\n",
    "# return the class with highest score for sentence\n",
    "def classify(sentence):\n",
    "    high_class = None\n",
    "    high_score = 0\n",
    "    # loop through our classes\n",
    "    for c in class_words.keys():\n",
    "        # calculate score of sentence for each class\n",
    "        score = calculate_class_score_commonality(sentence, c, show_details=False)\n",
    "        # keep track of highest score\n",
    "        if score > high_score:\n",
    "            high_class = c\n",
    "            high_score = score\n",
    "    return high_class, high_score\n",
    "\n",
    "import random\n",
    "import csv\n",
    "with open(r\"intents.json\", encoding=\"utf8\") as f:\n",
    "    reader = json.load(f)\n",
    "reader = reader\n",
    "def response(sentence):\n",
    "    result = classify(sentence)\n",
    "    dat = []\n",
    "    dat.append(sentence)\n",
    "    if (result[0] == None):\n",
    "        connection = mysql.connector.connect(host='localhost',\n",
    "                                             database='chatbot',\n",
    "                                             user='root',\n",
    "                                             password='')\n",
    "        cursor = connection.cursor()\n",
    "        mySql_insert_query = \"\"\"INSERT INTO question_table (question,status) \n",
    "                                VALUES (%s, %s) \"\"\"\n",
    "        recordTuple = (sentence,stat)\n",
    "        cursor.execute(mySql_insert_query, recordTuple)\n",
    "        connection.commit()    \n",
    "        return \"Sorry I didnt get you!\"\n",
    "    for i in reader['intents']:\n",
    "        if(i['tag']==result[0]):\n",
    "            return random.choice(i['responses'])\n",
    "\n",
    "  \n",
    "@app.route(\"/\")\n",
    "def home():    \n",
    "    return render_template(\"homee.html\") \n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():    \n",
    "    userText = request.args.get('msg')   \n",
    "    #return render_template(\"homee.html\", respondses=str(response(userText)), query=userText)\n",
    "    return str(response(userText)) \n",
    "if __name__ == \"__main__\":    \n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
